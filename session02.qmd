# Pop Gen In Conservation {#sec-s1}

*Session Presenters*

![](images/Presenters2.png)

## Required packages

```{r, warning=FALSE, message=FALSE}
#BiocManager::install("LEA")
#devtools::install_github("tdhock/directlabels")

library(dartRverse)
library(leaflet.minicharts)
library(LEA)
```

*make sure you have the packages installed, see* [Install dartRverse](install.qmd)

## Session Content

## Assessing Populations: structure and demographic history

### Load data and explore

read in data

this dataset is to assess population structure and diversity in Uperoleia crassa, from @jaya_population_2022.

```{r}
# gl <- dartR::gl.read.dart("Report_DUp20-4995_1_moreOrders_SNP_mapping_2.csv",
#                           ind.metafile = "Uperoleia_metadata.csv")

load('./data/session_2.RData') # data named gl

```

These are monsoonal tropical frogs, who are very common and abundant. They do interesting and weird stuff evolutionarily and reproductively

lets quickly look at our samples and populations

```{r}
gl.map.interactive(gl)
```

we have two populations - the Kimberley (IK) and the Top End (IT)

### Clean data

Clean up your dataset to remove the most egregiously bad loci and individuals this set of filtering can be used across analyses

```{r}
#Get rid of really poorly sequenced loci
#But don’t cut hard
gl.report.callrate(gl)
gl.1 <- gl.filter.callrate(gl, method = "loc", threshold = 0.8)

#Very low filter – this is only to get rid of your really bad individuals
gl.report.callrate(gl.1, method = "ind")
gl.2 <- gl.filter.callrate(gl.1, method = "ind", threshold = 0.25)

#Always run this after removing individuals – removes loci that are no longer variable
gl.3 <- gl.filter.monomorphs(gl.2)

#Get rid of unreliable loci
gl.report.reproducibility(gl.3)
gl.4 <- gl.filter.reproducibility(gl.3)

#Get rid of low and super high read depth loci
#do twice so you can zoom in
gl.report.rdepth(gl.4)
gl.5 <- gl.filter.rdepth(gl.4, lower = 0, upper = 25)
gl.clean <- gl.filter.rdepth(gl.5, lower = 8, upper = 17)

nInd(gl.clean)
nLoc(gl.clean)

#look at the data to see if you see any obvious issues and redo if you do.
plot(gl.clean)

rm(gl.1, gl.2, gl.3, gl.4, gl.5)



```

##### **this file is now your starting file for other filtering**

### Filtering

##### **Filtering data for population structure and phylogenetic analyses**

loci on the same fragment and missing data are not well supported in these analyses. Structure-like analyses dislikes missing data. So, we are going to filter even harder than normal here just so that we can see a result.

```{r, output = FALSE}
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.98)

#Remove minor alleles
#I usually set up the threshold so it is just 
# removing singletons to improve computation time
gl.report.maf(gl.1)
gl.2 <- gl.filter.maf(gl.1,  threshold = 1/(2*nInd(gl.1)))

#check that the data looks fairly clean
#this starts to show some obvious population banding
plot(gl.2)

#remove secondary SNPs on the same fragment
#Always do this as the last loci filter so that you’ve cut for quality 
# before you cut because there are two SNPs
gl.3 <- gl.filter.secondaries(gl.2)

#Filter on individuals. You can usually be a bit flexible at this point.
#individuals look a whole lot better now
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.3, method = "ind")
gl.4 <- gl.filter.callrate(gl.3, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.structure <- gl.filter.monomorphs(gl.4)

#this is your cleaned dataset for a population structure analysis              
plot(gl.structure)

nInd(gl.structure)
nLoc(gl.structure)

```

you can write this file out for various analyses that we will not go into here

`gl2structure(XXXX)` `gl2faststructure(XXXX)`

### Run SNMF (LEA)

This is a structure-like analysis called SNMF. This code is not all you would need to publish this result, but it is a good first look.

```{r, output = FALSE}

# LEA requires the genotype style file
gl2faststructure(gl.structure, outfile = "gl_structure.fstr",
                 outpath = './data/')
struct2geno("./data/gl_structure.fstr", ploidy = 2, FORMAT = 2) 
###this hates any loci with all heterozygotes

snmf.Sy.K1_8.10 = snmf("./data/gl_structure.fstr.geno", K = 1:8,
                       entropy = T, ploidy = 2, project="new", repetitions = 10)  

```

```{r}
plot(snmf.Sy.K1_8.10)
k <- 2 #chose best based on lowest cross entropy in graph
ce = cross.entropy(snmf.Sy.K1_8.10, K = k)
best <- which.min(ce)
par (mfrow = c(1,1))
barplot(t(Q(snmf.Sy.K1_8.10, K = k, run = best)), col = 1:k)


#Do a PCoA plot. I had these but they are also good for some stuff
pc <- gl.pcoa(gl.structure)
gl.pcoa.plot(pc, gl.structure)

rm(ce, gl.1, gl.2, gl.3, gl.4, snmf.Sy.K1_8.10, best, k, pc)


```

::: callout-note
## Exercise

![](images/task.png){#id .class width="48" height="48"} Have a go at altering various parameters and seeing how this changes your answers.

One thing that regularly changes with MAF filters is the amount of variance explained. Removing more minor alleles increases the variance explained in a PCoA. Think about why that would be the case.
:::

### Filtering for Tajima's D

Now we are going to look at how filtering can affect your understanding of demographic processes that population is Significant negative values of Tajima's D are due to an excess of rare alleles and are consistent with range expansion. Significant positive values are associated with population contraction. Significance (a P-value) cannot be calculated without a simulation. Please ask me how to do this using Hudson's ms program. Rare alleles matter! Lets look at how singletons impact our estimations of whether a population is expanding or contracting

This is a function written to calculate Tajima's D from SNP data. It will create the function in your global environment so that you can call it.

```{r}
get_tajima_D <- function(x){
  require(dartRverse) # possibly not needed for a function in an R package?
  
  # Find allele frequencies (p1 and p2) for every locus in every population
  allele_freqs <- gl.allele.freq(x)
  names(allele_freqs)[names(allele_freqs) == "frequency"] <- "p1"
  allele_freqs$p1 <- allele_freqs$p1 / 100
  allele_freqs$p2 <- 1 - allele_freqs$p1
  
  # Get the names of all the populations
  pops <- unique(allele_freqs$popn)
  
  #split each population
  allele_freqs_by_pop <- split(allele_freqs, allele_freqs$popn)
  
  # Internal function to calculate pi
  calc_pi <- function(allele_freqs) {
    n = allele_freqs$nobs * 2  # vector of n values
    pi_sqr <- allele_freqs$p1 ^ 2 + allele_freqs$p2 ^ 2
    h = (n / (n - 1)) * (1 - pi_sqr) # vector of values of h
    sum(h) # return pi, which is the sum of h across loci
  }
  
  get_tajima_D_for_one_pop <- function(allele_freqs_by_pop) {
    pi <- calc_pi(allele_freqs_by_pop)
    
# Calculate number of segregating sites, ignoring missing data (missing data will not appear in teh allele freq calcualtions)
    #S <- sum(!(allele_freqs_by_pop$p1 == 0 | allele_freqs_by_pop$p1 == 1))
    S <- sum(allele_freqs_by_pop$p1 >0 & allele_freqs_by_pop$p1 <1)
    if(S == 0) {
      warning("No segregating sites")
      data.frame(pi = NaN, 
                 S = NaN, 
                 D = NaN, 
                 Pval.normal = NaN, 
                 Pval.beta = NaN)
    }
    
    n <- mean(allele_freqs_by_pop$nobs * 2 )
    
    tmp <- 1:(n - 1)
    a1 <- sum(1/tmp)
    a2 <- sum(1/tmp^2)
    b1 <- (n + 1)/(3 * (n - 1))
    b2 <- 2 * (n^2 + n + 3)/(9 * n * (n - 1))
    c1 <- b1 - 1/a1
    c2 <- b2 - (n + 2)/(a1 * n) + a2/a1^2
    e1 <- c1/a1
    e2 <- c2/(a1^2 + a2)
    
    
    #calculate D and do beta testing
    D <- (pi - S/a1) / sqrt(e1 * S + e2 * S * (S - 1))
    Dmin <- (2/n - 1/a1)/sqrt(e2)
    Dmax <- ((n/(2*(n - 1))) - 1/a1)/sqrt(e2)
    tmp1 <- 1 + Dmin * Dmax
    tmp2 <- Dmax - Dmin
    a <- -tmp1 * Dmax/tmp2
    b <- tmp1 * Dmin/tmp2
  
    
    data.frame(pi = pi, 
               S = S, 
               D = D)
  }
  
  output <- do.call("rbind", lapply(allele_freqs_by_pop, 
                                    get_tajima_D_for_one_pop))
  data.frame(population = rownames(output), output, row.names = NULL)
}

```

We are going to focus on key filters and how they impact estimates

1.  Minor allele frequency (MAF) filtering
2.  No MAF allele filtering
3.  No MAF filtering but filtering on Read depth

#### 1. **MAF filtering**

We will start with our lightly cleaned data and filter this for the tajima's calculation

```{r}

#This function is written to calculate Tajima's D with a fair amount of missing data
#we are going to filter lightly here 
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.9)

#In this first round, we are going to actually remove singletons to see what happens
gl.report.maf(gl.1)
nLoc(gl.1)
gl.2 <- gl.filter.maf(gl.1,  threshold = 0.05)
nLoc(gl.2)

#check that the data looks fairly clean
#this starts ot show some obvious population banding
plot(gl.2)

#we are also going to remove secondary SNPs on the same fragment in this first round
gl.3 <- gl.filter.secondaries(gl.2)

#Filter on individuals. You can usually be a bit flexible at this point.
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.3, method = "ind")
gl.4 <- gl.filter.callrate(gl.3, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.D_withfiltering <- gl.filter.monomorphs(gl.4)


#calculate tajima's D with removing singletons and secondaries

D_w_filtering <- get_tajima_D(gl.D_withfiltering)
D_w_filtering

rm(gl.1, gl.2, gl.3, gl.4)

```

#### 2. **No MAF filtering**

Now lets try it where we *don't* remove singletons or secondaries

```{r}
#This function is written to calculate Tajima's D with a fair amount of missing data
#we are going to filter lightly here 
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.9)


#check that the data looks fairly clean
#this starts ot show some obvious population banding
plot(gl.1)

#Filter on individuals. You can usually be a bit flexible at this point.
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.1, method = "ind")
gl.2 <- gl.filter.callrate(gl.1, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.D_withOutfiltering <- gl.filter.monomorphs(gl.2)

rm(gl.1, gl.2)

D_wOUT_filtering <- get_tajima_D(gl.D_withOutfiltering)

```

#### 3. **No MAF filtering but filtering on Read depth**

Now lets try it where we *don't* remove singletons or secondaries AND we filter for read depth to remove singletons where we are not confident about their base calls

```{r}
#This function is written to calculate Tajima's D with a fair amount of missing data
# we are going to filter lightly here 
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.9)


#check that the data looks fairly clean
#this starts ot show some obvious population banding
plot(gl.1)

#filter to loci with a lower read depth, so that we are really confident
#that our base calls are correct
gl.report.rdepth(gl.1)
gl.2 <- gl.filter.rdepth(gl.1, lower = 12, upper = 17)


#Filter on individuals. You can usually be a bit flexible at this point.
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.2, method = "ind")
gl.3 <- gl.filter.callrate(gl.2, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.D_withOutfilteringRDepth <- gl.filter.monomorphs(gl.3)

rm(gl.1, gl.2, gl.3)

#calculate tajima's D with removing singletons and secondaries

D_wOUT_filtering_Rdepth <- get_tajima_D(gl.D_withOutfilteringRDepth)

```

### Results
##### *lets look at all three*

```{r}
D_w_filtering
D_wOUT_filtering
D_wOUT_filtering_Rdepth
```

::: callout-caution

### Interpreting results

For our Kimberley population, removing minor alleles give the actual wrong answer and suggests the population is contracting. This would have contradicted the rest of the findings of the paper, and been an artifact of the filtering process.

This shows how understanding the population genetic theory for a metric is crucial to useful analyses.

:::

## Further Study

still to come...
